import csv
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.model_selection import ShuffleSplit, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from matplotlib import pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering 
from sklearn.metrics import silhouette_samples, silhouette_score, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score
from scipy.cluster import hierarchy 
from scipy.spatial import distance_matrix 
from sklearn.cluster import DBSCAN 
from sklearn.preprocessing import LabelEncoder
from sklearn.manifold import TSNE
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as shc
import matplotlib.pyplot as plt
from palettable.colorbrewer.qualitative import Pastel1_7
from sklearn.ensemble import RandomForestClassifier





df=pd.read_csv('Frogs_MFCCs.csv')
df


df.info()


df.nunique()


df.isnull().sum()


df.describe()


sns.set_style("dark")
sns.countplot(x="Family", data=df, palette=sns.color_palette("husl", 8), saturation=10)


sns.set(rc={"font.style":"normal",
            "text.color":"black",
            "xtick.color":"black",
            "ytick.color":"black",
            "axes.labelcolor":"black",
            "axes.grid":False,
            'axes.labelsize':30,
            'figure.figsize':(12, 6),
            'xtick.labelsize':20,
            'ytick.labelsize':20})

sns.set(style="white",font_scale=0.8)


sns.set_style("dark")
sns.countplot(x="Genus", data=df, palette=sns.color_palette("husl", 8), 
              saturation=10, edgecolor=(0,0,0), linewidth=2)


# create data
names=list(df["Species"].unique())
sizes=[df["Species"].value_counts()[unique_class]*100/len(df["Species"]) for unique_class in names]
colors = Pastel1_7.hex_colors
explode = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)  # explode a slice if required

plt.pie(sizes, explode=explode, labels=names, colors=colors,
        autopct='%1.1f%%', shadow=True)
        
#draw a circle at the center of pie to make it look like a donut
centre_circle = plt.Circle((0,0), 0.50, color='black', fc='white',linewidth=0.70)
fig = plt.gcf()
fig.gca().add_artist(centre_circle)


# Set aspect ratio to be equal so that pie is drawn as a circle.
plt.axis('equal')
plt.show()


cat_feature_col=["Family", "Genus", "Species"]
for i in cat_feature_col:
    print(f"{i} : {df[i].unique()}")
    print(df[i].value_counts())
    print("----------------------------------------------------------------")


cont_feature_col = df.drop(cat_feature_col, axis=1).columns.to_list()
cont_df = df[cont_feature_col]
plt.figure(figsize=(20, 20))

for i in range(1, 24):
    plt.subplot(6, 4, i)
    sns.histplot(cont_df[cont_df.columns[i-1]], bins=14, kde=True, color="r", stat="density")
    plt.title(cont_df.columns[i-1])  # Agregar título para cada subplot

plt.tight_layout()
plt.show()


plt.figure(figsize=(30,10))
sns.boxplot(x="variable", y="value", data=pd.melt(cont_df.drop('RecordID', axis=1)))
plt.xticks(rotation=90)
plt.show()


# find the IQR
q1 = df[cont_feature_col].quantile(.25)
q3 = df[cont_feature_col].quantile(.75)
IQR = q3-q1

outliers_df = np.logical_or((df[cont_feature_col] < (q1 - 1.5 * IQR)), (df[cont_feature_col] > (q3 + 1.5 * IQR))) 

outlier_list=[]
total_outlier=[]
for col in list(outliers_df.columns):
    try:
        total_outlier.append(outliers_df[col].value_counts()[True])
        outlier_list.append((outliers_df[col].value_counts()[True] / outliers_df[col].value_counts().sum()) * 100)
    except:
        outlier_list.append(0)
        total_outlier.append(0)
        
outlier_list

outlier_df=pd.DataFrame(zip(list(outliers_df.columns), total_outlier, outlier_list), columns=['name of the column', 'total', 'outlier(%)'])

#see totally how many outliers in cont features
outlier_df.set_index('name of the column', inplace=True)
#del outlier_df.index.name
outlier_df


df_cont=df[cont_feature_col]
out_nan_df=df_cont[~outliers_df]
out_nan_df


for col in cont_feature_col:
  col_mean=df[col].mean() #calculate mean for each col
  out_nan_df[col]=out_nan_df[col].fillna(col_mean) #first convert outliers to Nan values then fill Nan's with col mean
  #df[cont_feature_col]=df_cont


df_only_cat=df.drop(columns=cont_feature_col)


#concat df_only_cat and clear cont_df of outliers
df_final=pd.concat([out_nan_df, df_only_cat], axis=1)


df_final


df_final.describe()





#minmax scaling
cont_cols = df_final.columns.difference(['Family','Genus', 'Species'])

scaler = MinMaxScaler()
cont_cols_df = pd.DataFrame(scaler.fit_transform(df_final[cont_cols]), 
                                columns=cont_cols, 
                                index=df_final.index)


le = LabelEncoder()

le.fit(df_final["Species"])
cat_cols_arr=le.transform(df_final["Species"])

cat_cols_df=pd.DataFrame(cat_cols_arr, columns=["Species"])


#merge cont&cat dfs
dff=pd.concat([cont_cols_df, cat_cols_df], axis=1)


# calculate correlation
df_corr = dff.corr()

# correlation matrix
sns.set(font_scale=0.8)
plt.figure(figsize=(20,16))
sns.heatmap(df_corr, annot=True, fmt=".4f",vmin=-1, vmax=1, linewidths=.5, cmap = sns.diverging_palette(145, 300, s=60, as_cmap=True))

#plt.yticks(rotation=0)
plt.show()


#X, y splitting
X_imp=df_final.iloc[:,:23]
y_imp=df_final.iloc[:,-1]

#feature importances
rf_clf = RandomForestClassifier(n_estimators = 100, max_depth=5)
rf_clf.fit(X_imp, y_imp)

pd.Series(rf_clf.feature_importances_, index = X_imp.columns).nlargest(24).plot(kind = 'pie',
                                                                                figsize = (8, 8),
                                                                                title = 'Feature importance from RandomForest', colormap='twilight', fontsize=10)





dff.drop(columns=["MFCCs_18"], inplace=True)








# run k-means for range of 10 clusters then analyse with elbow method
clusters = []

#drop target
X=dff.drop(columns=["Species"])
#y=dff["Family"]
y = dff.iloc[:,-1].values.reshape(1, -1)

#Sum of squared distances of samples to their closest cluster center.
for i in range(1, 11):
    km = KMeans(n_clusters=i).fit(X)
    clusters.append(km.inertia_)

fig, ax = plt.subplots(figsize=(12, 8))
sns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)
ax.set_title('--- Searching for Elbow ---')
ax.set_xlabel('Clusters')
ax.set_ylabel('Inertia')

plt.show()


#PCA for reducing dimensions to 3
pca = PCA(n_components=3)
pca_X = pca.fit_transform(X)


colors = np.array([x for x in 'bgrcmykbgr'])
#running k-means on resuts of pca
km_pca = KMeans(n_clusters=4).fit(pca_X)
fig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1], z=pca_X[:,2],color=colors[km_pca.labels_])
fig.show()


colors = np.array([x for x in 'ykbgrpcm'])
#running k-means on resuts of pca
km_pca_2 = KMeans(n_clusters=7).fit(pca_X)
fig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1], z=pca_X[:,2],color=colors[km_pca_2.labels_])
fig.show()


dff['KMEANS_4']=km_pca.labels_


dff





unique_genera = dff["Species"].unique()
colors = np.array([x for x in 'bgrcmykbgr'][:len(unique_genera)])  # Ajustar colores según el número de géneros

# Mapear los géneros a índices
color_indices = {genus: idx for idx, genus in enumerate(unique_genera)}
color_labels = dff["Species"].map(color_indices)

fig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1], z=pca_X[:,2], color=color_labels)
fig.show()








def silhouette_plot(X, y, n_clusters, ax=None):
    if ax is None:
        ax = plt.gca()

    # Compute the silhouette scores for each sample
    silhouette_avg = silhouette_score(X, y)
    sample_silhouette_values = silhouette_samples(X, y)

    y_lower = padding = 2
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        ith_cluster_silhouette_values = sample_silhouette_values[y == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0,
                         ith_cluster_silhouette_values,
                         facecolor=color,
                         edgecolor=color,
                         alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i + 1))

        # Compute the new y_lower for next plot
        y_lower = y_upper + padding

    ax.set_xlabel("The silhouette coefficient values")
    ax.set_ylabel("Cluster label")

    # The vertical line for average silhoutte score of all the values
    ax.axvline(x=silhouette_avg, c='r', alpha=0.8, lw=0.8, ls='-')
    ax.annotate('Average',
                xytext=(silhouette_avg, y_lower * 1.025),
                xy=(0, 0),
                ha='center',
                alpha=0.8,
                c='r')

    ax.set_yticks([])  # Clear the yaxis labels / ticks
    ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])
    ax.set_ylim(0, y_upper + 1)
    ax.set_xlim(-0.075, 1.0)
    return ax


# plot silhouette coefficients
plt.figure(figsize=(20,30))
for i in range(2,11):  
    agnes = AgglomerativeClustering(n_clusters=i, linkage='average')
    agnes_labels = agnes.fit_predict(pca_X)
    plt.subplot(5, 2, i-1)
    silhouette_plot(pca_X, agnes_labels, i)





#2 clusters
colors = np.array([x for x in 'ykbgrpcm'])
agnes2 = AgglomerativeClustering(n_clusters=2, linkage='average').fit(pca_X)

fig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1],z=pca_X[:,2], color=colors[agnes2.labels_])
fig.show()


#3 clusters
colors = np.array([x for x in 'grpcmykb'])
agnes4 = AgglomerativeClustering(n_clusters=3, linkage='average').fit(pca_X)

fig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1],z=pca_X[:,2], color=colors[agnes4.labels_])
fig.show()


plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(X, method='ward'))


dff['AGNES_4']=agnes4.labels_








# search for best parameters by using silhouette_score
score_list=[]
for eps in np.arange(0.5, 20, 0.5):
    for min_samples in range(3, 20):
        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)
        labels = db.labels_
        n = len(np.unique(labels))
        if n>1:
            score=silhouette_score(X, labels)
            score_list.append((score, (eps, min_samples)))
     
biggest_score = sorted(score_list)[-1]  
best_eps, best_min = biggest_score[1]
best_eps, best_min


# best model for DBSCAN
db_best = DBSCAN(eps=best_eps, min_samples=best_min).fit(X)

#best clusters
colors = np.array([x for x in 'rpcmykgb'])

fig = px.scatter_3d(x=pca_X[:,0], y=pca_X[:,1],z=pca_X[:,2], color=colors[db_best.labels_])
fig.show()


dff['DBSCAN_best']=db_best.labels_








# print metric for chosen models
models = [km_pca, agnes4, db_best] 
names = ["K-MEANS:", "AGNES:", "DBSCAN:"]


for i, model in enumerate(models):
    labels = model.labels_
    n = len(np.unique(labels))
    y_scaled = np.round(MinMaxScaler((0, n)).fit_transform(y)).ravel()
    
    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise_ = list(labels).count(-1)
    print(names[i])
    print('Estimated number of clusters: %d' % n_clusters_)
    print('Estimated number of noise points: %d' % n_noise_)
    print("Homogeneity: %0.3f" % homogeneity_score(y_scaled, labels))
    print("Fowlkes-Mallows score: %0.3f"
          % fowlkes_mallows_score(y_scaled, labels))
    print("Silhouette Coefficient: %0.3f"
          % silhouette_score(X, labels))
    print("\n######################################\n")





X_supervised = dff.drop(columns=["Species"])
y_supervised = dff["Species"]


from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_supervised, y_supervised, test_size=0.3, random_state=42)


from sklearn.model_selection import cross_val_score

rf_clf_supervised = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
# Use cross-validation to evaluate the supervised model
cv_scores_supervised = cross_val_score(rf_clf_supervised, X_supervised, y_supervised, cv=5, scoring='accuracy')

print(f"Cross-validation Accuracy (Supervised Model): {cv_scores_supervised.mean():.3f} (+/- {cv_scores_supervised.std():.3f})")

# Train the model on the full training data for later evaluation on the test set
rf_clf_supervised.fit(X_train, y_train)


from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

y_pred_supervised = rf_clf_supervised.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_supervised)
precision = precision_score(y_test, y_pred_supervised, average='weighted')
recall = recall_score(y_test, y_pred_supervised, average='weighted')
f1 = f1_score(y_test, y_pred_supervised, average='weighted')

print(f"Test Set Performance (Supervised Model):")
print(f"Accuracy: {accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")
print(f"F1-score: {f1:.3f}")


from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.ensemble import RandomForestClassifier

# 1. Create a new DataFrame for the baseline model
X_baseline = X_supervised.drop(columns=['KMEANS_4', 'AGNES_4', 'DBSCAN_best'])

# 2. Split this new baseline DataFrame and the target variable y_supervised
X_train_baseline, X_test_baseline, y_train_baseline, y_test_baseline = train_test_split(X_baseline, y_supervised, test_size=0.3, random_state=42)

# 3. Train a RandomForestClassifier model on the baseline training data
rf_clf_baseline = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)

# Use cross-validation to evaluate the baseline model
cv_scores_baseline = cross_val_score(rf_clf_baseline, X_baseline, y_supervised, cv=5, scoring='accuracy')

print(f"\nCross-validation Accuracy (Baseline Model): {cv_scores_baseline.mean():.3f} (+/- {cv_scores_baseline.std():.3f})")

# Train the model on the full training data for later evaluation on the test set
rf_clf_baseline.fit(X_train_baseline, y_train_baseline)

# 4. Predict the 'Genus' on the baseline test data
y_pred_baseline = rf_clf_baseline.predict(X_test_baseline)

# 5. Calculate and print the evaluation metrics for the baseline model
accuracy_baseline = accuracy_score(y_test_baseline, y_pred_baseline)
precision_baseline = precision_score(y_test_baseline, y_pred_baseline, average='weighted')
recall_baseline = recall_score(y_test_baseline, y_pred_baseline, average='weighted')
f1_baseline = f1_score(y_test_baseline, y_pred_baseline, average='weighted')

print(f"\nTest Set Performance (Baseline Model):")
print(f"Accuracy: {accuracy_baseline:.3f}")
print(f"Precision: {precision_baseline:.3f}")
print(f"Recall: {recall_baseline:.3f}")
print(f"F1-score: {f1_baseline:.3f}")




import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_blobs, make_moons
from sklearn.decomposition import PCA
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster, cophenet
from scipy.spatial.distance import pdist, squareform
from sklearn.preprocessing import StandardScaler
from collections import Counter
import pandas as pd
from scipy.io import loadmat


m = loadmat("dato_taller.mat", squeeze_me=True, struct_as_record=False)
X_train = np.asarray(m["x_entena"], dtype=np.float32)
X_test  = np.asarray(m["x_prueba"],  dtype=np.float32)
y_train = np.asarray(m["y_entrena"]).reshape(-1).astype(np.int64)


df_train = pd.DataFrame(X_train)
df_train["y"] = y_train


df_train.info()


df_train.describe().T


df_train["y"].value_counts()


corr = df_train.corr(numeric_only=True)

plt.matshow(corr)
plt.title("Matriz de correlaci贸n")
plt.colorbar()
plt.show()


df_features = df_train.drop(columns="y")

# Histograma conjunto
df_features.hist(bins=30, figsize=(12, 8))
plt.tight_layout()
plt.show()

# Boxplots globales
plt.figure(figsize=(10,5))
sns.boxplot(data=df_features, orient="h", fliersize=2)
plt.title("Distribuci贸n y posibles outliers")
plt.show()


#importantisimo normalizar datos!!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)


#Reducci贸n de dimensionalidad, explicar porque se deben reducir caracteristicas (columnas del dataset)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:,0], X_pca[:,1], c=y_train, cmap="coolwarm", s=10)
plt.title("Proyecci贸n PCA")
plt.show()


#sns.pairplot(df_train.sample(200), hue="y", diag_kind="kde", plot_kws={'s':10})
#plt.show()


#considerar eliminacion de outliers? 




